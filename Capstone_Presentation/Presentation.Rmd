---
title: "JHU Text Prediction Shiny Application"
subtitle: "Bill Lisse"
date: "May 20, 2023"
autosize: true
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## The Basics

This presentation features the Next Word Predict Shiny Application including an introduction to the application user interface and details about the text prediction
algorithm. The application will generate up to three different suggested words or phrases to complete any given character input in the text entry box.The benefit of this application is to speed writing and expected work(s) selection for techinal writers and bloggers.

The Next Word Predict app is located at:
<ul>
    <li><a target="_blank" href="https://wclisse.shinyapps.io/JHU_Capstone_Word_Predictor/">https://wclisse.shinyapps.io/JHU_Capstone_Word_Predictor/</a></li>
</ul>

The source code files can be found on GitHub:
<ul>
    <li><a target="_blank" href="https://github.com/Spyguy3/Data_Science_Capstone.git">https://github.com/Spyguy3/Data_Science_Capstone.git//</a></li>
</ul>


## Shiny Application

Word Predict is a Shiny application that uses a text
prediction algorithm to predict the next word(s) based on
text entered by a user.

The application will suggest the next word in a sentence
using an n-gram algorithm. An n-gram is a contiguous sequence
of *n* words from a given sequence of text.

The text used to build the predictive text model came from a
large corpus of blogs, news and twitter data. N-grams were
extracted from the corpus and then used to build the
predictive text model. 

Various methods were explored to improve speed and
accuracy using natural language processing and text mining
techniques.

## The Predictive Text Model

The predictive text model was built from a sample of
800,000 lines extracted from the large corpus of blogs,
news and twitter data. 

The sample data was then
tokenized and cleaned using the **tm** package and a number
of regular expressions using the **gsub** function. As
part of the cleaning process the data was converted to
lowercase, removed all non-ascii characters, URLs,
email addresses, Twitter handles, hash tags, ordinal numbers,
profane words, punctuation and whitespace. The data was
then split into tokens (n-grams). Tasks accomplished were:
1. Tokenization - identifying appropriate tokens such as words, punctuation, and numbers. Writing a function that takes a file as input and returns a tokenized version of it.
2. Profanity filtering - removing profanity and other words you do not want to predict.


## Application User Interface


As text is entered by the user, the algorithm iterates
from longest n-gram (4-gram) to shortest (2-gram) to
detect a match. The predicted next word is considered using
the longest, most frequent matching n-gram. The algorithm
makes use of a simple back-off strategy.

The predicted next word will be shown when the app
detects that you have finished typing one or more words.
When entering text, please allow a few seconds for the
output to appear. Use the slider tool to select up to
three next word predictions. The top prediction will be
shown first followed by the second and third likely
next words.
