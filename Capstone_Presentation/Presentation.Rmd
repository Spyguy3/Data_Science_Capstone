---
title: "Text Prediction Shiny Application"
subtitle: "Bill Lisse"
date: "May 20, 2023"
autosize: true
output: ioslides_presentation

---

css: slides.css
output: 
  ioslides_presentation: 
    widescreen: true
    smaller: true
    transition: 0.1
    self_contained: true
    logo: image/tandon_long_color.png
---

```{r setup, include=FALSE}
# This is an R setup chunk, containing default options applied to all other chunks
library(knitr)
# This sets the chunk default options
opts_chunk$set(cache=TRUE, collapse=TRUE, error=FALSE, prompt=TRUE, size="scriptsize")
# This sets the chunk display theme
knit_theme$set(knit_theme$get("acid"))
# This sets some display options
options(digits=3)
options(width=80)
```

```

## Summary

This presentation features the Next Word Predict Shiny Application including an introduction to the application user interface and details about the text prediction
algorithm. The application will generate up to three different suggested words or phrases to complete any given character input in the text entry box.The benefit of this application is to speed writing and expected work(s) selection for techinal writers and bloggers.

The Next Word Predict app is located at:
<ul>
    <li><a target="_blank" href="https://wclisse.shinyapps.io/NLP_Next_Word/">https://wclisse.shinyapps.io/NLP_Next_Word/</a></li>
</ul>

The source code files can be found on GitHub:
<ul>
    <li><a target="_blank" href="https://github.com/Spyguy3/Data_Science_Capstone.git">https://github.com/Spyguy3/Data_Science_Capstone.git//</a></li>
</ul>



## Shiny Application

Word Predict is a Shiny application that uses a text
prediction algorithm to predict the next word(s) based on
a text string entered by a user.

The application will suggest the next word in a string of text
using an n-gram algorithm. An n-gram is a contiguous sequence
of *n* words from a given sequence of text.

The text used to build the predictive text model came from a
large corpus of blogs, news and twitter data. N-grams were extracted from the corpus and then used to build the
predictive text model. 

Data was obtained from: 
<ul>
    <li><a target="_blank" href="https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip">Swiftkey</a></li>
</ul>


## The Predictive Text Model

The predictive text model was built from a sample of a large corpus of blogs,
news and twitter data from Swiftkey. 

The sample data was then tokenized and cleaned using the **tm** package and a number
of regular expressions using the **gsub** function. As part of the cleaning process the data was converted to lowercase, removed all non-ascii characters, URLs,
email addresses, Twitter handles, hash tags, ordinal numbers,
profane words, punctuation and whitespace. 

The data was then split into tokens (n-grams). Tasks accomplished were: (1) Tokenization - Writing a function that takes a file as input and returns a tokenized version of it.; (2) Profanity filtering - removing profanity and other bad words.


## Application User Interface


As text is entered by the user, the algorithm iterates
from longest n-gram (4-gram) to shortest (2-gram) to
detect a match. The predicted next word is considered using
the longest, most frequent matching n-gram. The algorithm
makes use of a simple back-off strategy.

The predicted next word will be shown when the app
detects that you have finished typing one or more words.
When entering text, please allow a few seconds for the
output to appear. Use the slider tool to select up to
three next word predictions. The top prediction will be
shown first followed by the second and third likely
next words.
